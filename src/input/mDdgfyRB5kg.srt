1
00:00:00,000 --> 00:00:03,816
[MUSIC PLAYING]

2
00:00:03,816 --> 00:00:05,914


3
00:00:05,914 --> 00:00:07,830
SHUBHIE PANICKER: My
name is Shubhie Panicker.

4
00:00:07,830 --> 00:00:10,782
I'm a software engineer working
on the web platform in Chrome.

5
00:00:10,782 --> 00:00:12,240
JASON MILLER: And
I'm Jason Miller.

6
00:00:12,240 --> 00:00:13,920
I'm a [INAUDIBLE] for Chrome.

7
00:00:13,920 --> 00:00:15,378
SHUBHIE PANICKER:
Our talk today is

8
00:00:15,378 --> 00:00:18,960
about a key strategy for
runtime performance of web apps,

9
00:00:18,960 --> 00:00:22,230
and that is scheduling of
JavaScript on the main thread,

10
00:00:22,230 --> 00:00:26,250
as well as approaches for moving
script off the main thread.

11
00:00:26,250 --> 00:00:28,950
Jason and I have both
been deep in this space

12
00:00:28,950 --> 00:00:33,650
exploring gaps in APIs for
what we are calling achieving

13
00:00:33,650 --> 00:00:36,042
get responsiveness guarantees.

14
00:00:36,042 --> 00:00:37,500
We're excited about
the opportunity

15
00:00:37,500 --> 00:00:40,200
here, both with
existing primitives

16
00:00:40,200 --> 00:00:43,190
as well as the new APIs
we will show in our talk.

17
00:00:43,190 --> 00:00:45,210
JASON MILLER: Right,
so to get started,

18
00:00:45,210 --> 00:00:47,560
let's illustrate this
problem space using a demo.

19
00:00:47,560 --> 00:00:50,430
So this is a simple application
that searches through photos

20
00:00:50,430 --> 00:00:51,870
as you type.

21
00:00:51,870 --> 00:00:53,670
And you can see here
with this JavaScript

22
00:00:53,670 --> 00:00:55,590
controlled red
spinner animation,

23
00:00:55,590 --> 00:00:57,450
it's doing a fair
amount of blocking work

24
00:00:57,450 --> 00:00:58,830
on the main thread.

25
00:00:58,830 --> 00:01:01,620
And while this is happening,
the app can't respond to input.

26
00:01:01,620 --> 00:01:03,990
So typing gets queued.

27
00:01:03,990 --> 00:01:06,770
Looking closer, what we see
if we pulled up the profiler

28
00:01:06,770 --> 00:01:10,500
is something like this--
a sequence of long tasks

29
00:01:10,500 --> 00:01:14,460
that block the main thread
and cause that input queuing.

30
00:01:14,460 --> 00:01:16,600
We can see this on a
simplified view here.

31
00:01:16,600 --> 00:01:19,050
So if we receive
input and we start

32
00:01:19,050 --> 00:01:21,470
doing some processing in
response to that input--

33
00:01:21,470 --> 00:01:24,330
say, searching photos
rendering some list items--

34
00:01:24,330 --> 00:01:26,400
we're skipping frames already.

35
00:01:26,400 --> 00:01:29,100
But in addition to that, if
we receive additional input

36
00:01:29,100 --> 00:01:32,460
while that task is running,
it will get queued,

37
00:01:32,460 --> 00:01:37,050
and it only is able to execute
once that has completed.

38
00:01:37,050 --> 00:01:39,770
So this is data
captured from real users

39
00:01:39,770 --> 00:01:41,470
on real websites in the wild.

40
00:01:41,470 --> 00:01:44,270
And it shows a breakdown of
where Chrome was spending

41
00:01:44,270 --> 00:01:46,481
its time while handling input.

42
00:01:46,481 --> 00:01:48,230
So there's a lot of
interesting data here,

43
00:01:48,230 --> 00:01:50,105
but we don't really have
time to get into it.

44
00:01:50,105 --> 00:01:52,370
The main thing to look
at is the amount of time

45
00:01:52,370 --> 00:01:55,190
that we're spending in
this V8.execute task.

46
00:01:55,190 --> 00:01:57,020
That is Chrome
running JavaScript

47
00:01:57,020 --> 00:01:58,400
during touch handling.

48
00:01:58,400 --> 00:02:00,200
And it's clearly the
biggest contributor

49
00:02:00,200 --> 00:02:03,680
to touch input latency, both on
average and also in the worst

50
00:02:03,680 --> 00:02:04,652
case.

51
00:02:04,652 --> 00:02:06,860
SHUBHIE PANICKER: So a
problem with our search as you

52
00:02:06,860 --> 00:02:08,930
type example app is
that there's just

53
00:02:08,930 --> 00:02:11,890
a lot of different types
of work happening here.

54
00:02:11,890 --> 00:02:15,010
And all of these
different work have what

55
00:02:15,010 --> 00:02:17,480
we are calling rate deadlines.

56
00:02:17,480 --> 00:02:20,240
So for example, the user is
typing in that search box.

57
00:02:20,240 --> 00:02:22,190
Their input has
to be responsive.

58
00:02:22,190 --> 00:02:24,170
There's ongoing
animations on the page.

59
00:02:24,170 --> 00:02:26,954
They have to render
consistently and smoothly.

60
00:02:26,954 --> 00:02:28,370
And then there's
the heavy lifting

61
00:02:28,370 --> 00:02:31,460
of fetching search results,
post-processing, preparing,

62
00:02:31,460 --> 00:02:33,980
rendering these
search results in time

63
00:02:33,980 --> 00:02:37,490
so that it's relevant to
the user's typed in query.

64
00:02:37,490 --> 00:02:40,040
The difficulty is that
it's hard for apps

65
00:02:40,040 --> 00:02:43,040
to balance these
competing needs to reason

66
00:02:43,040 --> 00:02:45,410
about all these different
deadlines and keeping

67
00:02:45,410 --> 00:02:48,256
everything meeting
these timelines.

68
00:02:48,256 --> 00:02:49,880
JASON MILLER: Right,
so we have a bunch

69
00:02:49,880 --> 00:02:51,140
of different types of work.

70
00:02:51,140 --> 00:02:54,440
And each of those types of
work has a different deadline.

71
00:02:54,440 --> 00:02:56,810
And what we need to be
able to work through this

72
00:02:56,810 --> 00:02:58,119
is priorities.

73
00:02:58,119 --> 00:02:59,660
SHUBHIE PANICKER:
So there's a couple

74
00:02:59,660 --> 00:03:02,420
high level approaches
to try and achieve

75
00:03:02,420 --> 00:03:03,680
responsiveness guarantees.

76
00:03:03,680 --> 00:03:07,250
The first approach can
just be doing less work.

77
00:03:07,250 --> 00:03:10,940
And there are ways of doing this
such as in an infinite feed,

78
00:03:10,940 --> 00:03:13,250
you might only render
what's visible.

79
00:03:13,250 --> 00:03:15,680
We just saw a strategy with
the virtual-scroller talk

80
00:03:15,680 --> 00:03:16,790
right now.

81
00:03:16,790 --> 00:03:19,370
Now this is not always possible.

82
00:03:19,370 --> 00:03:22,790
Modern apps often just
have a ton of work to do.

83
00:03:22,790 --> 00:03:26,120
So a second strategy
here is chunking up work

84
00:03:26,120 --> 00:03:30,550
and prioritizing
these chunks of work.

85
00:03:30,550 --> 00:03:34,090
In practice, though, this
is also very difficult.

86
00:03:34,090 --> 00:03:37,390
It can be impractical to achieve
this manually on your own

87
00:03:37,390 --> 00:03:38,920
as an app developer.

88
00:03:38,920 --> 00:03:42,130
And we think there's a real
opportunity here for frameworks

89
00:03:42,130 --> 00:03:44,380
to step in and help their users.

90
00:03:44,380 --> 00:03:46,480
Frameworks are in
a great position

91
00:03:46,480 --> 00:03:50,860
to ensure chunking and
prioritizing of work.

92
00:03:50,860 --> 00:03:53,920
So stepping back a
bit, what we need here

93
00:03:53,920 --> 00:03:58,450
is some way to provide our
chunks of work, our tasks,

94
00:03:58,450 --> 00:04:02,410
to a system that can hold
them, say, in a task queue,

95
00:04:02,410 --> 00:04:04,690
and then the system
can make good decisions

96
00:04:04,690 --> 00:04:07,430
about when to take tasks
out of the task queue

97
00:04:07,430 --> 00:04:10,210
and execute them at an
appropriate time based

98
00:04:10,210 --> 00:04:12,520
on everything that's going on.

99
00:04:12,520 --> 00:04:16,301
And this is the
definition of a scheduler.

100
00:04:16,301 --> 00:04:18,550
JASON MILLER: So Google Maps
is a really great example

101
00:04:18,550 --> 00:04:20,740
of an application that
uses a scheduler to keep

102
00:04:20,740 --> 00:04:22,750
its interactions smooth.

103
00:04:22,750 --> 00:04:25,960
This app has to manage multiple
different types of interactions

104
00:04:25,960 --> 00:04:26,500
and events.

105
00:04:26,500 --> 00:04:28,510
And these can all
happen concurrently.

106
00:04:28,510 --> 00:04:30,730
They do this by
scheduling all work

107
00:04:30,730 --> 00:04:33,310
and giving a much higher
priority to input response

108
00:04:33,310 --> 00:04:34,100
tasks.

109
00:04:34,100 --> 00:04:35,030
We can see that here.

110
00:04:35,030 --> 00:04:36,760
So let's say I'm
panning the map.

111
00:04:36,760 --> 00:04:38,920
And as I'm panning,
additional tiles

112
00:04:38,920 --> 00:04:41,350
are coming into the viewport
and need to be loaded.

113
00:04:41,350 --> 00:04:43,810
However, if I stop
panning and I pull up

114
00:04:43,810 --> 00:04:45,730
the drawer at the
bottom, all of a sudden,

115
00:04:45,730 --> 00:04:49,490
that is far and away the most
high priority task to execute.

116
00:04:49,490 --> 00:04:54,142
So those map tiles being loaded
need to be de-prioritized.

117
00:04:54,142 --> 00:04:56,350
SHUBHIE PANICKER: So a key
aspect of a good scheduler

118
00:04:56,350 --> 00:05:00,350
is its ability to execute
work at the best time.

119
00:05:00,350 --> 00:05:02,020
And this is an
appropriate time based

120
00:05:02,020 --> 00:05:04,660
on everything that's going
on, various factors like,

121
00:05:04,660 --> 00:05:07,390
what's the type of the task,
what's important to the user

122
00:05:07,390 --> 00:05:10,630
right now, what's the overall
state of the application,

123
00:05:10,630 --> 00:05:14,290
what's the internal state
of the browser, et cetera.

124
00:05:14,290 --> 00:05:17,110
So to understand this
notion of best time,

125
00:05:17,110 --> 00:05:21,220
we have to step down a level and
look at the browser's rendering

126
00:05:21,220 --> 00:05:22,360
pipeline.

127
00:05:22,360 --> 00:05:25,360
The browser is periodically
bumping frames, typically

128
00:05:25,360 --> 00:05:29,740
every 16 milliseconds for a 60
frames per second display rate.

129
00:05:29,740 --> 00:05:34,330
And each frame has a set of
things that happen in sequence.

130
00:05:34,330 --> 00:05:37,450
For instance, we have request
animation frames followed

131
00:05:37,450 --> 00:05:39,460
by style, layout, and paint.

132
00:05:39,460 --> 00:05:42,250
In Chrome, input handlers
are aligned right

133
00:05:42,250 --> 00:05:44,840
before request animation
frame callbacks.

134
00:05:44,840 --> 00:05:48,550
So the point here is that
there is limited time

135
00:05:48,550 --> 00:05:53,110
to do the urgent work that needs
to happen in the current frame.

136
00:05:53,110 --> 00:05:54,940
And then the app has
to immediately start

137
00:05:54,940 --> 00:05:58,120
thinking about preparing
for that next frame.

138
00:05:58,120 --> 00:06:00,520
And the third type of
work here is idle work,

139
00:06:00,520 --> 00:06:03,190
which might be left over
in the current frame

140
00:06:03,190 --> 00:06:05,320
or there might be
plenty of idle time

141
00:06:05,320 --> 00:06:09,010
if no frames are being rendered.

142
00:06:09,010 --> 00:06:10,710
So this is the
terminology we are using

143
00:06:10,710 --> 00:06:12,390
for these three types of work.

144
00:06:12,390 --> 00:06:15,750
We have user-blocking tasks
for the current frame.

145
00:06:15,750 --> 00:06:18,180
This is typically
to provide the user

146
00:06:18,180 --> 00:06:21,090
an immediate acknowledgment
of what they are doing.

147
00:06:21,090 --> 00:06:25,830
So in our example app, this
might be keeping that typing

148
00:06:25,830 --> 00:06:29,100
interactive in the search box,
keeping those animations going

149
00:06:29,100 --> 00:06:30,030
on the page.

150
00:06:30,030 --> 00:06:32,160
Overall, keeping the
page responsive overall--

151
00:06:32,160 --> 00:06:34,110
buttons should be toggleable.

152
00:06:34,110 --> 00:06:36,930
Default work is this
next category of work.

153
00:06:36,930 --> 00:06:38,610
This is typically user visible.

154
00:06:38,610 --> 00:06:42,690
And this is preparing for the
next frame or a future frame.

155
00:06:42,690 --> 00:06:44,370
And in our example,
this would be

156
00:06:44,370 --> 00:06:48,770
the work of the post-processing,
preparing the search results,

157
00:06:48,770 --> 00:06:50,610
rendering them in time.

158
00:06:50,610 --> 00:06:53,080
And finally, the third
category, idle work.

159
00:06:53,080 --> 00:06:56,610
This is typically work
that is not user visible.

160
00:06:56,610 --> 00:06:58,260
This can be at the
end of the frame

161
00:06:58,260 --> 00:07:00,480
or if no frames are
being rendered-- things

162
00:07:00,480 --> 00:07:03,540
like analytics, backups,
syncs, or indexing.

163
00:07:03,540 --> 00:07:06,630


164
00:07:06,630 --> 00:07:10,990
So on the right here,
we've listed some existing

165
00:07:10,990 --> 00:07:14,680
primitives, existing ways
how a developer might

166
00:07:14,680 --> 00:07:18,070
be able to submit work
to the browser to target

167
00:07:18,070 --> 00:07:19,490
these priority levels.

168
00:07:19,490 --> 00:07:22,370
So for user-blocking,
input handler, and request

169
00:07:22,370 --> 00:07:25,210
animation frames
are great for this.

170
00:07:25,210 --> 00:07:28,300
It's also worth noting
that micro tasks are suited

171
00:07:28,300 --> 00:07:30,430
for user-blocking urgent work.

172
00:07:30,430 --> 00:07:32,920
They do not yield
to the event loop.

173
00:07:32,920 --> 00:07:35,590
And we've seen some bad
cases where developers

174
00:07:35,590 --> 00:07:38,230
are accidentally doing
non-urgent or large amounts

175
00:07:38,230 --> 00:07:41,680
of work without realizing
it's blocking rendering.

176
00:07:41,680 --> 00:07:43,780
The second thing,
default. We have

177
00:07:43,780 --> 00:07:46,240
things like setTimeout
zero, postMessage.

178
00:07:46,240 --> 00:07:48,340
These are really hacks
and work arounds.

179
00:07:48,340 --> 00:07:50,890
There isn't a real
primitive here.

180
00:07:50,890 --> 00:07:52,870
And we are working
to fill this gap.

181
00:07:52,870 --> 00:07:55,780
And finally, for idle,
requestIdleCallback

182
00:07:55,780 --> 00:07:57,580
is a great API.

183
00:07:57,580 --> 00:08:01,840
So JavaScript schedulers
can be built today

184
00:08:01,840 --> 00:08:03,780
using these primitives.

185
00:08:03,780 --> 00:08:06,130
Now while it's possible
to build a scheduling

186
00:08:06,130 --> 00:08:10,210
system in JavaScript,
they suffer from gaps

187
00:08:10,210 --> 00:08:16,310
primarily because they don't
have enough control on signals

188
00:08:16,310 --> 00:08:18,370
to properly control scheduling.

189
00:08:18,370 --> 00:08:21,560
So we'll go through
some examples.

190
00:08:21,560 --> 00:08:23,920
So for example, we've
seen JavaScript schedulers

191
00:08:23,920 --> 00:08:26,744
are trying to estimate
the frame deadline.

192
00:08:26,744 --> 00:08:28,660
So they're doing a whole
bunch of bookkeeping,

193
00:08:28,660 --> 00:08:29,770
trying to guess at it.

194
00:08:29,770 --> 00:08:31,811
But they're doing it poorly
because it's just not

195
00:08:31,811 --> 00:08:35,260
possible to do this well without
knowing browser internals.

196
00:08:35,260 --> 00:08:39,000
So we are considering
exposing an API for that.

197
00:08:39,000 --> 00:08:43,240
isInputPending is a really
useful signal for schedulers,

198
00:08:43,240 --> 00:08:46,270
and we are actively
exploring an API.

199
00:08:46,270 --> 00:08:48,490
Then there's other
coordination work.

200
00:08:48,490 --> 00:08:52,780
So for example, handling
fetch response priorities

201
00:08:52,780 --> 00:08:54,760
is pretty relevant.

202
00:08:54,760 --> 00:08:57,190
If you're doing urgent
work for the current frame,

203
00:08:57,190 --> 00:09:00,640
you don't want your low
priority fetch responses

204
00:09:00,640 --> 00:09:02,860
to come in and interrupt that.

205
00:09:02,860 --> 00:09:05,980
In practice, though,
there's a lot of other work

206
00:09:05,980 --> 00:09:07,850
that's happening in the browser.

207
00:09:07,850 --> 00:09:10,270
The browser might
initiate various callbacks

208
00:09:10,270 --> 00:09:12,820
such as ready state
change for XHR

209
00:09:12,820 --> 00:09:15,220
or a postMessage might
come in from a worker.

210
00:09:15,220 --> 00:09:17,440
There's internal work
like [INAUDIBLE]..

211
00:09:17,440 --> 00:09:20,770
And it's just not possible
to codify priorities

212
00:09:20,770 --> 00:09:24,350
for all of this and
[INAUDIBLE] signals.

213
00:09:24,350 --> 00:09:28,840
So this got us thinking, how
about moving the scheduler one

214
00:09:28,840 --> 00:09:32,770
level down and integrating it
directly with the browser's

215
00:09:32,770 --> 00:09:36,550
event loop where we already
have most of these signals

216
00:09:36,550 --> 00:09:38,620
and a lot of great information?

217
00:09:38,620 --> 00:09:41,320
And this would solve
an additional problem.

218
00:09:41,320 --> 00:09:43,300
That is this
coordination problem

219
00:09:43,300 --> 00:09:45,920
between multiple
parties in the app.

220
00:09:45,920 --> 00:09:49,540
If you have third party content,
or embedded libraries or legacy

221
00:09:49,540 --> 00:09:53,290
code, or even other frameworks,
they can all co-exist and use

222
00:09:53,290 --> 00:09:58,930
the same day scheduling system
with consistent priorities.

223
00:09:58,930 --> 00:10:04,110
So this is a very early sketch
of what an API might look like.

224
00:10:04,110 --> 00:10:08,520
The key thing here is a set
of global task use targeting

225
00:10:08,520 --> 00:10:09,730
each priority level.

226
00:10:09,730 --> 00:10:11,760
And so this is really
simple and straightforward

227
00:10:11,760 --> 00:10:15,540
compared to using a
myriad different APIs.

228
00:10:15,540 --> 00:10:17,160
The second thing
is we think it will

229
00:10:17,160 --> 00:10:20,850
be useful to have a notion
of user-defined task

230
00:10:20,850 --> 00:10:22,470
queues, a virtual task queues.

231
00:10:22,470 --> 00:10:25,380
And this would give
developers more control

232
00:10:25,380 --> 00:10:30,450
over managing a group of tasks
and doing bulk operations

233
00:10:30,450 --> 00:10:33,600
like updating priority,
canceling all the tasks,

234
00:10:33,600 --> 00:10:37,440
or flushing the task used
if the app is going away.

235
00:10:37,440 --> 00:10:40,077
JASON MILLER: Right, so
here we can see a simplified

236
00:10:40,077 --> 00:10:41,910
version of that map
scheduler that we looked

237
00:10:41,910 --> 00:10:44,700
at using this task queue API.

238
00:10:44,700 --> 00:10:48,470
So first, we hook into the
user blocking and default task

239
00:10:48,470 --> 00:10:51,610
queues just to give ourselves a
high and a low priority queue.

240
00:10:51,610 --> 00:10:53,940
And then we start listening
for pointermoves events.

241
00:10:53,940 --> 00:10:55,815
And each time we receive
one of these events,

242
00:10:55,815 --> 00:10:58,440
we enqueue a pan
task with coordinates

243
00:10:58,440 --> 00:11:00,060
at that pointermove.

244
00:11:00,060 --> 00:11:02,610
The pan task translates
the map tiles, obviously.

245
00:11:02,610 --> 00:11:06,090
And then it might, let's say,
enqueue a low priority task

246
00:11:06,090 --> 00:11:10,110
to detect any tiles that
have moved into the viewport

247
00:11:10,110 --> 00:11:12,360
and potentially
load those tiles.

248
00:11:12,360 --> 00:11:15,780
The thing to note here is if we
receive a new pointermove event

249
00:11:15,780 --> 00:11:19,290
before we've invoked this
loadMoreTiles task, that

250
00:11:19,290 --> 00:11:21,870
would be given a higher priority
than loading more tiles.

251
00:11:21,870 --> 00:11:23,203
And that's exactly what we want.

252
00:11:23,203 --> 00:11:26,440
We give higher priority
to input-driven tasks.

253
00:11:26,440 --> 00:11:28,332
And let's say the
team behind Maps

254
00:11:28,332 --> 00:11:30,540
needed to track analytics
or do something in response

255
00:11:30,540 --> 00:11:32,790
to pan gestures, they'll be
a good case for something

256
00:11:32,790 --> 00:11:35,637
like an idle priority task.

257
00:11:35,637 --> 00:11:37,970
SHUBHIE PANICKER: So here you
can see most of the frames

258
00:11:37,970 --> 00:11:40,470
are in green and getting
rendered in time.

259
00:11:40,470 --> 00:11:43,310
And this is what a well
scheduled system looks like.

260
00:11:43,310 --> 00:11:44,840
The work is chunked up.

261
00:11:44,840 --> 00:11:47,150
There is high priority
work happening

262
00:11:47,150 --> 00:11:50,060
at the beginning of every
frame, followed by style,

263
00:11:50,060 --> 00:11:52,070
layout, paint in
purple and green,

264
00:11:52,070 --> 00:11:55,700
just immediately followed by
default priority work in yellow

265
00:11:55,700 --> 00:11:59,900
to prepare for the next frame,
as well as idle priority work

266
00:11:59,900 --> 00:12:01,560
being properly interleaved.

267
00:12:01,560 --> 00:12:04,580
And this time is
adequately utilized.

268
00:12:04,580 --> 00:12:06,240
Next.

269
00:12:06,240 --> 00:12:09,650
So all the APIs
proposals we showed today

270
00:12:09,650 --> 00:12:11,330
are super early stage.

271
00:12:11,330 --> 00:12:14,750
We actually don't know what the
end game here is going to be.

272
00:12:14,750 --> 00:12:19,730
This is a really great time
to give us feedback and help

273
00:12:19,730 --> 00:12:22,160
us chart the course here.

274
00:12:22,160 --> 00:12:23,900
For web developers,
we really think

275
00:12:23,900 --> 00:12:26,660
that there is an opportunity
here with improved scheduling,

276
00:12:26,660 --> 00:12:30,320
even if they're just properly
using existing primitives.

277
00:12:30,320 --> 00:12:32,180
For framework authors,
we want to urge

278
00:12:32,180 --> 00:12:34,790
you to consider a
scheduling system

279
00:12:34,790 --> 00:12:38,990
and collaborate with us now to
develop the right set of APIs

280
00:12:38,990 --> 00:12:40,280
in this space.

281
00:12:40,280 --> 00:12:43,760
React's work on concurrent
and time slicing

282
00:12:43,760 --> 00:12:46,010
has proven that
frameworks can really

283
00:12:46,010 --> 00:12:49,340
play a good role in
terms of helping apps

284
00:12:49,340 --> 00:12:52,710
improve responsiveness of apps.

285
00:12:52,710 --> 00:12:54,450
And we're already
working with React

286
00:12:54,450 --> 00:12:57,180
and actively looking
to form partnerships

287
00:12:57,180 --> 00:12:59,650
with other frameworks and apps.

288
00:12:59,650 --> 00:13:01,770
This is a link to
our GitHub repo.

289
00:13:01,770 --> 00:13:04,410
Filing issues on the
repo is a great way

290
00:13:04,410 --> 00:13:06,750
to get that feedback
dialogue going.

291
00:13:06,750 --> 00:13:07,710
JASON MILLER: Right.

292
00:13:07,710 --> 00:13:12,120
So what about work that
can't be chunked, though?

293
00:13:12,120 --> 00:13:15,210
What if we have a bunch of
JavaScript we need to execute,

294
00:13:15,210 --> 00:13:18,030
and it's really difficult or
even potentially impossible

295
00:13:18,030 --> 00:13:20,460
to break that work up?

296
00:13:20,460 --> 00:13:22,891
Here's an example that
illustrates what I mean.

297
00:13:22,891 --> 00:13:24,390
Let's say we have
a text editor that

298
00:13:24,390 --> 00:13:26,490
does something like
live JavaScript bundling

299
00:13:26,490 --> 00:13:27,660
as you type.

300
00:13:27,660 --> 00:13:30,630
If I load in a decent
amount of code here,

301
00:13:30,630 --> 00:13:32,620
things start to be
a little bit slow.

302
00:13:32,620 --> 00:13:34,080
So every time the
bundling process

303
00:13:34,080 --> 00:13:37,800
kicks in response to my input,
it blocks the main thread.

304
00:13:37,800 --> 00:13:39,690
And this causes the
cursor to freeze

305
00:13:39,690 --> 00:13:42,640
and it queues up my text input
until bundling is completed.

306
00:13:42,640 --> 00:13:44,880
And this really disrupts
the typing experience.

307
00:13:44,880 --> 00:13:48,480
You can see that in the CPU
profile here on the right.

308
00:13:48,480 --> 00:13:51,600
So it would be really
difficult to break

309
00:13:51,600 --> 00:13:53,580
that work up into 50
millisecond chunks,

310
00:13:53,580 --> 00:13:54,760
and that's for two reasons.

311
00:13:54,760 --> 00:13:57,990
First, I didn't write
any of the Bundler code,

312
00:13:57,990 --> 00:14:01,980
so modifying that would be a lot
of work, particularly for me.

313
00:14:01,980 --> 00:14:04,770
Plus there is a whole bunch
of different libraries

314
00:14:04,770 --> 00:14:07,530
that are being used to actually
make these things happen,

315
00:14:07,530 --> 00:14:08,400
those dependencies.

316
00:14:08,400 --> 00:14:11,550
And downloading, parsing, and
evaluating those dependencies

317
00:14:11,550 --> 00:14:13,650
on the main thread blocks.

318
00:14:13,650 --> 00:14:17,689
So using background threads lets
us offload that work and get it

319
00:14:17,689 --> 00:14:19,980
off the main thread so the
main thread can sort of just

320
00:14:19,980 --> 00:14:22,620
keep handling input.

321
00:14:22,620 --> 00:14:25,020
There's a few use cases that
lend themselves extremely

322
00:14:25,020 --> 00:14:26,340
well to this approach.

323
00:14:26,340 --> 00:14:29,190
If you're building a computer
aided design tool, a game,

324
00:14:29,190 --> 00:14:31,440
or doing encoding,
these are great places

325
00:14:31,440 --> 00:14:34,050
to just start with threads.

326
00:14:34,050 --> 00:14:37,364
Same thing for AI,
machine learning, crypto.

327
00:14:37,364 --> 00:14:39,280
If these are the types
of things you're doing,

328
00:14:39,280 --> 00:14:41,220
you should start here.

329
00:14:41,220 --> 00:14:44,652
In the browser, our primitive
for threading is the worker.

330
00:14:44,652 --> 00:14:46,110
So if you haven't
used Workers, you

331
00:14:46,110 --> 00:14:49,090
haven't used them in a while,
they're basically threads.

332
00:14:49,090 --> 00:14:50,715
They have a simple
messaging interface,

333
00:14:50,715 --> 00:14:52,530
so you can send a
message to the worker,

334
00:14:52,530 --> 00:14:54,330
and you can receive
a message back.

335
00:14:54,330 --> 00:14:57,180
They have no DOM access
whatsoever and a very limited

336
00:14:57,180 --> 00:15:00,709
global scope, kind of just
fetch and module stuff.

337
00:15:00,709 --> 00:15:02,250
And they shipped
around 10 years ago,

338
00:15:02,250 --> 00:15:05,530
and they're available
essentially everywhere.

339
00:15:05,530 --> 00:15:08,040
So the API for workers
looks like this.

340
00:15:08,040 --> 00:15:10,110
You will instantiate
the worker constructor

341
00:15:10,110 --> 00:15:12,260
and pass up the
name of a script.

342
00:15:12,260 --> 00:15:13,950
And then we can
listen for messages

343
00:15:13,950 --> 00:15:15,760
coming back out of that worker.

344
00:15:15,760 --> 00:15:17,680
And we can send messages
down to the worker.

345
00:15:17,680 --> 00:15:20,320
So here, we're sending it
a message that's an object.

346
00:15:20,320 --> 00:15:22,725
And this describes that we
would like to, say, invoke

347
00:15:22,725 --> 00:15:24,810
a computeHash function.

348
00:15:24,810 --> 00:15:27,300
And we're going to pass
it the contents of a file,

349
00:15:27,300 --> 00:15:29,410
expressed here as
an array buffer.

350
00:15:29,410 --> 00:15:31,560
The second argument to
postMessage is interesting.

351
00:15:31,560 --> 00:15:33,350
This tells the browser
to, rather than

352
00:15:33,350 --> 00:15:37,230
structured cloning the array
buffer, it will transfer it in.

353
00:15:37,230 --> 00:15:39,760
Finally once computeHash
has completed,

354
00:15:39,760 --> 00:15:42,690
it will say, postMessage
back to our thread

355
00:15:42,690 --> 00:15:45,727
and will be dropped into the
message handler on line 3.

356
00:15:45,727 --> 00:15:47,310
SHUBHIE PANICKER:
So under the covers,

357
00:15:47,310 --> 00:15:50,869
this postMessage of the data
is incurring a serialization

358
00:15:50,869 --> 00:15:51,660
on the main thread.

359
00:15:51,660 --> 00:15:53,520
And it's getting
queued up, hopping over

360
00:15:53,520 --> 00:15:56,460
to a worker thread,
followed by deserialization.

361
00:15:56,460 --> 00:15:59,460
And end-to-end, this
is called a thread hop.

362
00:15:59,460 --> 00:16:02,370
And this thread hop has
a cost, and primarily

363
00:16:02,370 --> 00:16:04,470
from the data being
subject to what

364
00:16:04,470 --> 00:16:06,090
is called structured
cloning, which

365
00:16:06,090 --> 00:16:09,060
is a copying behavior while
recursing the JavaScript

366
00:16:09,060 --> 00:16:09,840
object.

367
00:16:09,840 --> 00:16:16,270
The size of the data is relevant
to the cost of the thread hop.

368
00:16:16,270 --> 00:16:19,450
So one downside of
the postMessage API

369
00:16:19,450 --> 00:16:22,120
is that it doesn't have
a notion of statefulness

370
00:16:22,120 --> 00:16:23,935
between the request
and the response.

371
00:16:23,935 --> 00:16:26,290
So if you make a whole
bunch of requests,

372
00:16:26,290 --> 00:16:28,210
you'll get a whole
bunch of responses back.

373
00:16:28,210 --> 00:16:32,077
And it's hard to correlate
those responses to requests.

374
00:16:32,077 --> 00:16:32,910
JASON MILLER: Right.

375
00:16:32,910 --> 00:16:35,076
So we've seen how to
communicate with a worker using

376
00:16:35,076 --> 00:16:35,970
postMessage.

377
00:16:35,970 --> 00:16:38,190
There's actually a number
of ways you can do this.

378
00:16:38,190 --> 00:16:40,320
A second way would be
to use MessageChannel.

379
00:16:40,320 --> 00:16:42,320
MessageChannel is something
you can instantiate,

380
00:16:42,320 --> 00:16:43,560
and you get back two ports.

381
00:16:43,560 --> 00:16:46,830
You can pass your other port to
some other contexts like a tab

382
00:16:46,830 --> 00:16:50,550
or a frame or a worker, and you
can message between the two.

383
00:16:50,550 --> 00:16:53,370
They have the same
interface, as we just saw.

384
00:16:53,370 --> 00:16:55,260
Another one would
be BroadcastChannel.

385
00:16:55,260 --> 00:16:57,090
This is like a
MessageChannel that's

386
00:16:57,090 --> 00:16:59,160
shared to all
contexts associated

387
00:16:59,160 --> 00:17:01,440
with an origin, so all
tabs, frames, workers,

388
00:17:01,440 --> 00:17:02,520
service worker.

389
00:17:02,520 --> 00:17:04,740
And all you do is you
instantiate a BroadcastChannel

390
00:17:04,740 --> 00:17:06,579
with a channel keyword.

391
00:17:06,579 --> 00:17:09,060
And you message without
having to pass ports around.

392
00:17:09,060 --> 00:17:10,976
Soon we're actually going
to have a fourth way

393
00:17:10,976 --> 00:17:11,680
to communicate.

394
00:17:11,680 --> 00:17:13,140
And this is
transferable streams.

395
00:17:13,140 --> 00:17:15,597
It lends itself really well
to things like audio and video

396
00:17:15,597 --> 00:17:18,180
where the format you would want
to use to express these things

397
00:17:18,180 --> 00:17:19,650
is streaming.

398
00:17:19,650 --> 00:17:23,520
The thing with all of these APIs
is that they're message-based.

399
00:17:23,520 --> 00:17:27,210
And based on some of the common
usage patterns that we've seen

400
00:17:27,210 --> 00:17:29,010
and what we've heard
from developers,

401
00:17:29,010 --> 00:17:32,820
we think there might be a case
here for a higher level API.

402
00:17:32,820 --> 00:17:36,510
So we've seen solutions to this
in userland through libraries

403
00:17:36,510 --> 00:17:40,170
like comlink, greenlet,
workerize, via.js.

404
00:17:40,170 --> 00:17:43,320
These all help coordinate
messaging across boundaries

405
00:17:43,320 --> 00:17:45,990
by abstracting that
postMessage using something

406
00:17:45,990 --> 00:17:47,474
called proxying.

407
00:17:47,474 --> 00:17:49,140
SHUBHIE PANICKER: So
messaging certainly

408
00:17:49,140 --> 00:17:51,941
improves over a postMessage.

409
00:17:51,941 --> 00:17:52,440
I'm sorry.

410
00:17:52,440 --> 00:17:53,960
Proxying improves
over postMessage,

411
00:17:53,960 --> 00:17:55,950
but it comes with a
number of downsides.

412
00:17:55,950 --> 00:17:58,390
Every method call
to a proxied object

413
00:17:58,390 --> 00:18:02,280
incurs the cost of a thread hop,
and this can come as a surprise

414
00:18:02,280 --> 00:18:03,630
to developers.

415
00:18:03,630 --> 00:18:08,070
Platform gaps can cause
memory leaks in these APIs.

416
00:18:08,070 --> 00:18:12,270
These APIs don't really have a
notion of a backing threadpool

417
00:18:12,270 --> 00:18:15,480
or a concept of managing
threads and resizing the pool.

418
00:18:15,480 --> 00:18:19,530
Embedded libraries are not
able to share the same thread

419
00:18:19,530 --> 00:18:20,690
or threadpool.

420
00:18:20,690 --> 00:18:24,630
And for complex APIs, it can
be impractical to recreate

421
00:18:24,630 --> 00:18:28,630
this API surface cross thread.

422
00:18:28,630 --> 00:18:30,700
So this raises the
question, is there

423
00:18:30,700 --> 00:18:34,000
an opportunity here for better
integration with the browser?

424
00:18:34,000 --> 00:18:37,620
Is there an opportunity to
provide a more compelling API?

425
00:18:37,620 --> 00:18:39,054
JASON MILLER: Right.

426
00:18:39,054 --> 00:18:41,470
We think there might be a use
case here for something that

427
00:18:41,470 --> 00:18:43,370
looks something like this.

428
00:18:43,370 --> 00:18:45,790
So here, we're passing
the name of a function

429
00:18:45,790 --> 00:18:47,920
in some other context
and some arguments

430
00:18:47,920 --> 00:18:50,800
to a theoretical
postTask method.

431
00:18:50,800 --> 00:18:53,170
This postTask method
would return a promise

432
00:18:53,170 --> 00:18:55,930
that eventually resolves to the
return value of that function

433
00:18:55,930 --> 00:18:57,690
somewhere else.

434
00:18:57,690 --> 00:19:02,620
This abstract code helps us move
from a message passing model

435
00:19:02,620 --> 00:19:04,510
to a more task-oriented model.

436
00:19:04,510 --> 00:19:06,826


437
00:19:06,826 --> 00:19:08,950
SHUBHIE PANICKER: So in
looking at the requirements

438
00:19:08,950 --> 00:19:13,090
for a better API, we
considered other platforms.

439
00:19:13,090 --> 00:19:15,730
We looked at iOS
and Android, which

440
00:19:15,730 --> 00:19:19,240
have plenty of precedent for
usage of background threads.

441
00:19:19,240 --> 00:19:22,720
In particular, iOS has an API
called Grand Central Dispatch,

442
00:19:22,720 --> 00:19:24,970
which is a very stable
well-proven API that's

443
00:19:24,970 --> 00:19:27,016
loved by iOS developers.

444
00:19:27,016 --> 00:19:29,140
Android, amongst other
things, has something called

445
00:19:29,140 --> 00:19:32,440
AsyncTask, which is a
very minimal, clean API.

446
00:19:32,440 --> 00:19:35,800
We talked to framework
developers and experts

447
00:19:35,800 --> 00:19:39,520
in usage of these APIs
who were deeply familiar

448
00:19:39,520 --> 00:19:42,560
with the pitfalls,
and we learned things.

449
00:19:42,560 --> 00:19:46,220
Some key things we learned in
terms of the basic requirements

450
00:19:46,220 --> 00:19:50,390
we want for our model is,
number one, good ergonomics,

451
00:19:50,390 --> 00:19:52,590
a way for developers
to offload work

452
00:19:52,590 --> 00:19:54,340
by just thinking in
terms of submitting

453
00:19:54,340 --> 00:19:58,130
task versus coordinating
over threads.

454
00:19:58,130 --> 00:19:59,870
Secondly, a native
threadpool that's

455
00:19:59,870 --> 00:20:02,990
shareable with embedded
libraries and other parties

456
00:20:02,990 --> 00:20:04,050
in the app.

457
00:20:04,050 --> 00:20:06,860
And finally, a system-controlled
thread management,

458
00:20:06,860 --> 00:20:10,160
where the system can be in
control of making decisions

459
00:20:10,160 --> 00:20:13,590
on resizing the threadpool or
decisions on where to run which

460
00:20:13,590 --> 00:20:14,090
tasks.

461
00:20:14,090 --> 00:20:16,980


462
00:20:16,980 --> 00:20:19,170
So we set off on a
path towards building

463
00:20:19,170 --> 00:20:23,220
a basic task queue-based API
inspired by Grand Central

464
00:20:23,220 --> 00:20:24,480
Dispatch.

465
00:20:24,480 --> 00:20:26,920
And a naive API
might look like this.

466
00:20:26,920 --> 00:20:30,080
Let's say you have three
tasks, A, B, and C.

467
00:20:30,080 --> 00:20:32,040
And let's say each one
depends on the results

468
00:20:32,040 --> 00:20:36,300
of the previous task, and
we can submit these tasks

469
00:20:36,300 --> 00:20:38,340
from the main thread
over to worker threads.

470
00:20:38,340 --> 00:20:40,620
And then we'll start
getting responses back.

471
00:20:40,620 --> 00:20:45,075
So here for three tasks, we paid
the cost of six thread hops.

472
00:20:45,075 --> 00:20:48,020


473
00:20:48,020 --> 00:20:50,880
There's a few downsides
here and gotchas.

474
00:20:50,880 --> 00:20:53,790
So for one, these
thread hops can be

475
00:20:53,790 --> 00:20:55,860
expensive on lower-end devices.

476
00:20:55,860 --> 00:20:57,600
And depending on
the data size, it

477
00:20:57,600 --> 00:21:01,870
can be up to 15 milliseconds,
and this can add up.

478
00:21:01,870 --> 00:21:03,660
This means that
if these hops are

479
00:21:03,660 --> 00:21:05,880
in the path of user
interaction, this

480
00:21:05,880 --> 00:21:10,020
can add up to multiple
frames worth of latency now.

481
00:21:10,020 --> 00:21:12,420
On Android, we've actually
seen this in practice

482
00:21:12,420 --> 00:21:16,080
in the real world in
their usage of AsyncTask.

483
00:21:16,080 --> 00:21:19,320
So one conclusion
here is this notion

484
00:21:19,320 --> 00:21:22,770
of default posting back results
to the main thread is not

485
00:21:22,770 --> 00:21:24,420
a good idea.

486
00:21:24,420 --> 00:21:27,300
Besides the latency issue,
it can cause congestion

487
00:21:27,300 --> 00:21:28,860
from queue buildups.

488
00:21:28,860 --> 00:21:31,260
And then you might remember
from our earlier main thread

489
00:21:31,260 --> 00:21:33,930
scheduling talk, we're
doing all this work

490
00:21:33,930 --> 00:21:37,860
to carefully chunk up our work
and execute our high priority

491
00:21:37,860 --> 00:21:39,400
and our default priority work.

492
00:21:39,400 --> 00:21:42,990
And all these postMessages
coming in at random times

493
00:21:42,990 --> 00:21:45,310
messes with main
thread scheduling.

494
00:21:45,310 --> 00:21:48,750
A second thing to note here
is that default posting,

495
00:21:48,750 --> 00:21:52,620
even to the current
thread, can be pretty bad.

496
00:21:52,620 --> 00:21:56,620
And we saw this in Grand Central
Dispatch with their dispatch

497
00:21:56,620 --> 00:21:59,159
at current queue API.

498
00:21:59,159 --> 00:22:01,200
JASON MILLER: So this
brings us to a new proposal

499
00:22:01,200 --> 00:22:04,110
we have that incorporates
some of our learnings

500
00:22:04,110 --> 00:22:05,850
from other platforms.

501
00:22:05,850 --> 00:22:09,960
It lets developers avoid sending
data back to the main thread.

502
00:22:09,960 --> 00:22:13,830
It lets you chain tasks
together without data transfer,

503
00:22:13,830 --> 00:22:16,950
and pay the return
cost only once.

504
00:22:16,950 --> 00:22:19,290
It also minimizes
thread hops using

505
00:22:19,290 --> 00:22:21,600
a built-in sticky threadpool.

506
00:22:21,600 --> 00:22:23,940
What we want is the experience
that you see up here

507
00:22:23,940 --> 00:22:25,431
on the right.

508
00:22:25,431 --> 00:22:26,430
So let's dive into that.

509
00:22:26,430 --> 00:22:28,687
If we revisit the Code Editor
that we showed earlier,

510
00:22:28,687 --> 00:22:30,270
the one that bundles
JavaScript as you

511
00:22:30,270 --> 00:22:33,240
type, if we do this
using Task Worklet,

512
00:22:33,240 --> 00:22:34,830
we can leverage some
of these features

513
00:22:34,830 --> 00:22:38,010
to improve performance
fairly considerably.

514
00:22:38,010 --> 00:22:40,080
Because Task Worklet
avoids transferring

515
00:22:40,080 --> 00:22:43,500
data between threads, the
bundling and minifying tasks

516
00:22:43,500 --> 00:22:45,630
in this demo can
actually all reuse

517
00:22:45,630 --> 00:22:49,230
the same AST that is generated
from that initial parse task.

518
00:22:49,230 --> 00:22:52,200
In the end, only the
resulting minified code,

519
00:22:52,200 --> 00:22:54,030
which is a relatively
small string,

520
00:22:54,030 --> 00:22:57,530
actually gets sent back
to the main thread.

521
00:22:57,530 --> 00:22:59,530
So the implementation
looks something like this.

522
00:22:59,530 --> 00:23:01,530
First, we create
a Worklet module,

523
00:23:01,530 --> 00:23:03,830
and that registers
named task processors.

524
00:23:03,830 --> 00:23:06,070
These are just classes
with a process method.

525
00:23:06,070 --> 00:23:07,900
Then over on the
main thread, we can

526
00:23:07,900 --> 00:23:12,730
coordinate that data
flow using this postTask.

527
00:23:12,730 --> 00:23:14,710
So we're going to
parse the code and then

528
00:23:14,710 --> 00:23:17,950
pass that resulting AST through
the bundle and minify tasks.

529
00:23:17,950 --> 00:23:19,450
And the important
thing to note here

530
00:23:19,450 --> 00:23:23,710
is that none of these variables
are actually holding values.

531
00:23:23,710 --> 00:23:27,070
These are just pointers to data
that exists in the threadpool.

532
00:23:27,070 --> 00:23:28,720
Data transfer back
to the main thread

533
00:23:28,720 --> 00:23:31,510
only happens when
we await the result

534
00:23:31,510 --> 00:23:34,870
property of that last task.

535
00:23:34,870 --> 00:23:37,210
So doing this in a typical
workers implementation

536
00:23:37,210 --> 00:23:40,060
would normally take
six hops, as we saw.

537
00:23:40,060 --> 00:23:41,320
We executed three tasks.

538
00:23:41,320 --> 00:23:44,170
We need to pass a message down
and backup for each of them.

539
00:23:44,170 --> 00:23:46,960
In Task Worklet, this
is only two thread

540
00:23:46,960 --> 00:23:50,800
hops because we can
transfer data between tasks.

541
00:23:50,800 --> 00:23:53,390
Task Worklet is also
backed by a threadpool.

542
00:23:53,390 --> 00:23:56,050
So let's say we start
off with a task that

543
00:23:56,050 --> 00:23:58,100
produces a large set of images.

544
00:23:58,100 --> 00:24:02,590
When we post a task with some
of those images as its argument,

545
00:24:02,590 --> 00:24:04,420
it will attempt to
run in the thread

546
00:24:04,420 --> 00:24:06,820
where that data is
already available.

547
00:24:06,820 --> 00:24:09,370
So data is never transferred
between threads in this case,

548
00:24:09,370 --> 00:24:11,500
and at least a few
are thread hops.

549
00:24:11,500 --> 00:24:13,090
To take advantage
of pooling, though,

550
00:24:13,090 --> 00:24:15,250
if there's no optimal
thread available,

551
00:24:15,250 --> 00:24:18,130
we will resort to transferring
data between threads

552
00:24:18,130 --> 00:24:20,890
in order to get parallelization.

553
00:24:20,890 --> 00:24:22,475
And then finally,
let's say the result

554
00:24:22,475 --> 00:24:24,100
that we're looking
for here is actually

555
00:24:24,100 --> 00:24:25,870
just a comparison of
the number of cats

556
00:24:25,870 --> 00:24:28,000
versus the number of
dog photos, since that's

557
00:24:28,000 --> 00:24:30,520
what's important in the end.

558
00:24:30,520 --> 00:24:32,230
In this case, the
only thing we ever

559
00:24:32,230 --> 00:24:35,200
transfer back to the main
thread is a single integer.

560
00:24:35,200 --> 00:24:37,925
And as you can imagine,
that's extremely cheap.

561
00:24:37,925 --> 00:24:39,550
So we've been thinking
a lot about what

562
00:24:39,550 --> 00:24:42,010
the future of web development
off the main thread

563
00:24:42,010 --> 00:24:45,100
might look like today we have
libraries like Comlink that

564
00:24:45,100 --> 00:24:47,470
use reflection to
kind of emulate

565
00:24:47,470 --> 00:24:49,390
the interface of some
code running in a worker

566
00:24:49,390 --> 00:24:51,848
so that it could be called from
the main thread seamlessly.

567
00:24:51,848 --> 00:24:55,210
In the future, we think we might
move towards a Task Worklet

568
00:24:55,210 --> 00:24:58,750
model where developers approach
multi-threaded web programming

569
00:24:58,750 --> 00:24:59,890
in sort of a different way.

570
00:24:59,890 --> 00:25:02,980
You have a threadpool that's
managed automatically, named

571
00:25:02,980 --> 00:25:05,920
tasks, and this
concept of a task graph

572
00:25:05,920 --> 00:25:10,430
that optimizes
execution and data flow.

573
00:25:10,430 --> 00:25:12,962
So this is a really
early proposal.

574
00:25:12,962 --> 00:25:14,920
We are looking for
feedback, and we are looking

575
00:25:14,920 --> 00:25:16,790
for real-world use cases.

576
00:25:16,790 --> 00:25:18,820
There is an implementation
available in Chromium

577
00:25:18,820 --> 00:25:22,320
behind the Experimental
Web Platform Features flag.

578
00:25:22,320 --> 00:25:24,580
And also we have Polyfill
and some source code

579
00:25:24,580 --> 00:25:27,390
and demos available
at this GitHub repo.

580
00:25:27,390 --> 00:25:30,105
There will be a link at the end
of the presentation as well.

581
00:25:30,105 --> 00:25:31,480
SHUBHIE PANICKER:
So there's been

582
00:25:31,480 --> 00:25:35,140
a lot of interest in this idea
of multi-threaded JavaScript

583
00:25:35,140 --> 00:25:36,620
over the last couple years.

584
00:25:36,620 --> 00:25:38,620
There have been several
independent explorations

585
00:25:38,620 --> 00:25:40,600
by various frameworks and apps.

586
00:25:40,600 --> 00:25:43,090
So we dug into this
in the last few months

587
00:25:43,090 --> 00:25:46,510
to understand how far can we
get with just using the worker

588
00:25:46,510 --> 00:25:50,170
API as a way to
achieve threadedness.

589
00:25:50,170 --> 00:25:53,860
And to set some context
here, a new worker

590
00:25:53,860 --> 00:25:56,500
doesn't just spin
up a raw OS thread.

591
00:25:56,500 --> 00:25:59,380
It actually creates its
own JavaScript environment

592
00:25:59,380 --> 00:26:01,650
on top of this.

593
00:26:01,650 --> 00:26:04,540
And part of that is what's
called a V8 isolate, which

594
00:26:04,540 --> 00:26:07,270
has a non-trivial
weight in addition

595
00:26:07,270 --> 00:26:09,130
to the weight of the OS thread.

596
00:26:09,130 --> 00:26:12,760
A key implication here
is that the worker,

597
00:26:12,760 --> 00:26:15,190
by creating its own
JavaScript environment,

598
00:26:15,190 --> 00:26:18,550
is not able to share data or
code with the main thread.

599
00:26:18,550 --> 00:26:21,160
And this is fundamentally
different from background

600
00:26:21,160 --> 00:26:26,560
threads on other platforms
and other languages.

601
00:26:26,560 --> 00:26:30,480
So this has implications
in terms of using Workers

602
00:26:30,480 --> 00:26:32,320
in a mainstream way.

603
00:26:32,320 --> 00:26:35,580
And by that I mean when the
worker is in the path of user

604
00:26:35,580 --> 00:26:36,600
interaction.

605
00:26:36,600 --> 00:26:39,540
In particular, we looked at
two app development models

606
00:26:39,540 --> 00:26:40,800
using worker.

607
00:26:40,800 --> 00:26:43,850
The first one is doing state
management in a worker,

608
00:26:43,850 --> 00:26:46,680
and this is where you can do
the heavy lifting, business

609
00:26:46,680 --> 00:26:48,480
logic-y stuff in a worker.

610
00:26:48,480 --> 00:26:51,960
And the second model
goes even further

611
00:26:51,960 --> 00:26:54,570
and does the bulk of
rendering in the worker.

612
00:26:54,570 --> 00:26:57,810
Now while worker doesn't
have access to the DOM,

613
00:26:57,810 --> 00:27:00,000
there are libraries
like WorkerDOM.

614
00:27:00,000 --> 00:27:02,950
So you can do virtual
DOM updates in the worker

615
00:27:02,950 --> 00:27:06,250
and then ferry the [? diffs ?]
back to the main thread.

616
00:27:06,250 --> 00:27:10,470
So real apps have been
built using these models.

617
00:27:10,470 --> 00:27:13,230
However, there are some
significant challenges

618
00:27:13,230 --> 00:27:15,990
that we want to
highlight here if you're

619
00:27:15,990 --> 00:27:18,570
planning to go down this route.

620
00:27:18,570 --> 00:27:24,540
The first thing is that it's
hard to have synchronous access

621
00:27:24,540 --> 00:27:27,690
to a worker, but real apps
need synchronous access

622
00:27:27,690 --> 00:27:28,812
to their app state.

623
00:27:28,812 --> 00:27:30,270
So what this means
is sometimes you

624
00:27:30,270 --> 00:27:32,353
want to look up your app
state on the main thread,

625
00:27:32,353 --> 00:27:35,570
and sometimes you might update
that app state on a worker.

626
00:27:35,570 --> 00:27:38,880
And this means you now have
to maintain and replicate

627
00:27:38,880 --> 00:27:42,390
this app state in both
places and synchronize it

628
00:27:42,390 --> 00:27:43,630
continuously.

629
00:27:43,630 --> 00:27:47,640
And this has a cost in
terms of thread hops.

630
00:27:47,640 --> 00:27:50,910
The second thing here is
that the worker has to be

631
00:27:50,910 --> 00:27:54,420
bootstrapped with all the
script and modules that it needs

632
00:27:54,420 --> 00:27:57,300
because, like we said, it
cannot share code with the main

633
00:27:57,300 --> 00:27:57,960
thread.

634
00:27:57,960 --> 00:28:02,630
And this has implications
for startup delay.

635
00:28:02,630 --> 00:28:07,460
So we run benchmarks to dig
into this base cost of a worker.

636
00:28:07,460 --> 00:28:10,790
And these are some numbers
from a medium Android device.

637
00:28:10,790 --> 00:28:13,250
Startup takes upwards
of 10 millisecond.

638
00:28:13,250 --> 00:28:16,050
And this is, again,
a Chrome on Android.

639
00:28:16,050 --> 00:28:20,240
A thread hop varies anywhere
from one to 15 milliseconds,

640
00:28:20,240 --> 00:28:25,420
depending on the device and
the size and type of the data.

641
00:28:25,420 --> 00:28:27,890
Look out for a
blog post that will

642
00:28:27,890 --> 00:28:31,250
be accompanying this talk
in the next week or two.

643
00:28:31,250 --> 00:28:33,590
And we will have some detailed
links to our benchmarks

644
00:28:33,590 --> 00:28:35,820
and data there.

645
00:28:35,820 --> 00:28:39,330
We also set up more
realistic benchmarks.

646
00:28:39,330 --> 00:28:41,220
We built apps that
were representing

647
00:28:41,220 --> 00:28:43,800
the app development models
we mentioned, that is,

648
00:28:43,800 --> 00:28:47,490
the state management in a worker
and rendering in a worker.

649
00:28:47,490 --> 00:28:51,940
And we did a ton of runs
on real mobile devices,

650
00:28:51,940 --> 00:28:53,760
both with and without worker.

651
00:28:53,760 --> 00:28:56,160
And we looked at a
variety of metrics,

652
00:28:56,160 --> 00:28:59,610
everything from loading
metrics, memory metrics,

653
00:28:59,610 --> 00:29:03,990
to rendering metrics, such as
frame rate and input latency.

654
00:29:03,990 --> 00:29:07,600
And we approximated input
latency using cycle time.

655
00:29:07,600 --> 00:29:11,730
So again, the blog post will
have more details on this.

656
00:29:11,730 --> 00:29:16,130
But I do want to highlight
one bit of interesting data.

657
00:29:16,130 --> 00:29:19,960
So this is basically
showing runs

658
00:29:19,960 --> 00:29:22,750
with an app that
is representative

659
00:29:22,750 --> 00:29:24,190
of rendering in a worker.

660
00:29:24,190 --> 00:29:28,630
The red are runs with worker,
and blue runs without worker.

661
00:29:28,630 --> 00:29:32,140
So what we are seeing
here is that on worker, we

662
00:29:32,140 --> 00:29:35,650
are seeing a higher and
more improved frame rate.

663
00:29:35,650 --> 00:29:37,720
But on the flip
side, we are also

664
00:29:37,720 --> 00:29:41,200
seeing a higher input latency.

665
00:29:41,200 --> 00:29:43,150
So there is a
fundamental trade-off

666
00:29:43,150 --> 00:29:49,060
here between improved
smoothness versus user latency.

667
00:29:49,060 --> 00:29:52,880
Workers are able to free up the
main thread by offloading work.

668
00:29:52,880 --> 00:29:54,850
And so they can free
up that main thread

669
00:29:54,850 --> 00:30:00,070
to focus on rendering, and
less grip means fewer long task

670
00:30:00,070 --> 00:30:01,900
hiccups on the main thread.

671
00:30:01,900 --> 00:30:04,780
Again, on the flip
side, input latency

672
00:30:04,780 --> 00:30:07,000
suffers from thread hops.

673
00:30:07,000 --> 00:30:09,730
And the worker environment
is a limited environment

674
00:30:09,730 --> 00:30:10,870
and doesn't have APIs.

675
00:30:10,870 --> 00:30:12,280
And it's not just the DOM.

676
00:30:12,280 --> 00:30:14,930
There are many other APIs as
that are still not available,

677
00:30:14,930 --> 00:30:16,450
like media, audio, et cetera.

678
00:30:16,450 --> 00:30:17,260
JASON MILLER: Yeah.

679
00:30:17,260 --> 00:30:18,940
So the key thing to
take away from this

680
00:30:18,940 --> 00:30:23,260
is, workers might be able to
make your rendering smoother,

681
00:30:23,260 --> 00:30:26,290
but they might do it at the
expense of a bit of input

682
00:30:26,290 --> 00:30:27,670
delay.

683
00:30:27,670 --> 00:30:30,400
There's cases, though, where
this is completely worth it.

684
00:30:30,400 --> 00:30:31,910
So AMPscript is a great example.

685
00:30:31,910 --> 00:30:34,030
AMPscript renders
using workers in order

686
00:30:34,030 --> 00:30:37,330
to sandbox, potentially
misbehaving JavaScript.

687
00:30:37,330 --> 00:30:39,490
Slower or problematic
code that's

688
00:30:39,490 --> 00:30:42,340
running in the worker
in this emulated DOM

689
00:30:42,340 --> 00:30:45,792
can't negatively impact
the AMP document.

690
00:30:45,792 --> 00:30:47,500
And so for AMP, the
benefits they get out

691
00:30:47,500 --> 00:30:50,920
of sandboxing untrusted code
far outweigh the latency

692
00:30:50,920 --> 00:30:53,890
that they get from
transferring events.

693
00:30:53,890 --> 00:30:57,040
So we wanted to summarize
when they use workers,

694
00:30:57,040 --> 00:31:00,920
but it turns out there's
no perfect rubric for this.

695
00:31:00,920 --> 00:31:03,340
So there's a couple of
hints you can use though.

696
00:31:03,340 --> 00:31:05,530
If you have code that
blocks for a long time,

697
00:31:05,530 --> 00:31:07,810
if you have code with
small inputs and outputs,

698
00:31:07,810 --> 00:31:10,880
or something that follows the
simple request response model,

699
00:31:10,880 --> 00:31:13,910
you might be in a position
to start off with workers.

700
00:31:13,910 --> 00:31:16,360
However, if you have code
that relies on the DOM

701
00:31:16,360 --> 00:31:19,090
or is directly in the
path of input response,

702
00:31:19,090 --> 00:31:22,000
or just code that needs
really minimal overhead,

703
00:31:22,000 --> 00:31:24,250
you might want to start off
with a different solution.

704
00:31:24,250 --> 00:31:26,770
You could approach
workers later.

705
00:31:26,770 --> 00:31:29,500
When adopting a threaded
approach to state management,

706
00:31:29,500 --> 00:31:32,200
make sure that your state
management and business

707
00:31:32,200 --> 00:31:35,620
logic outweighs the cost
of creating a worker

708
00:31:35,620 --> 00:31:37,270
and sending and
receiving messages.

709
00:31:37,270 --> 00:31:41,009
Make sure that your worker
is pulling its own weight.

710
00:31:41,009 --> 00:31:42,550
So we're at the
beginning of a fairly

711
00:31:42,550 --> 00:31:47,140
major shift in how applications
are developed for the web.

712
00:31:47,140 --> 00:31:50,080
We're excited to explore
new possibilities

713
00:31:50,080 --> 00:31:52,670
for effective scheduling
and threading.

714
00:31:52,670 --> 00:31:54,719
And we hope that
all of you are too.

715
00:31:54,719 --> 00:31:56,260
SHUBHIE PANICKER:
And so we just want

716
00:31:56,260 --> 00:31:58,690
to leave you with some of these
key messages from our talk

717
00:31:58,690 --> 00:31:59,530
today.

718
00:31:59,530 --> 00:32:01,720
It's hard to achieve
responsiveness guarantees

719
00:32:01,720 --> 00:32:04,210
because there's so much work
happening in modern apps.

720
00:32:04,210 --> 00:32:06,400
And we think scheduling
is a compelling strategy

721
00:32:06,400 --> 00:32:07,630
for tackling this.

722
00:32:07,630 --> 00:32:10,180
There's an opportunity here
for improved scheduling

723
00:32:10,180 --> 00:32:13,570
with existing primitives
as well as new primitives.

724
00:32:13,570 --> 00:32:15,580
And frameworks are
in a good position

725
00:32:15,580 --> 00:32:17,290
to play a big role here.

726
00:32:17,290 --> 00:32:20,620
In terms of offloading
work from the main thread,

727
00:32:20,620 --> 00:32:22,930
you can think of using
worker as an extension

728
00:32:22,930 --> 00:32:25,300
to better main
thread scheduling.

729
00:32:25,300 --> 00:32:28,490
Some types of work are better
suited to worker than others.

730
00:32:28,490 --> 00:32:32,020
And we think new APIs
like Task Worklet

731
00:32:32,020 --> 00:32:37,060
are going to be compelling to
utilize worker for scheduling.

732
00:32:37,060 --> 00:32:39,120
So that's about it.

733
00:32:39,120 --> 00:32:41,950
We'll have a blog post
coming with more details.

734
00:32:41,950 --> 00:32:45,450
These are, again, the
links to the GitHub repos.

735
00:32:45,450 --> 00:32:49,190
Issues on the repos are
very welcome and appreciated

736
00:32:49,190 --> 00:32:52,050
and a great way too
for the feedback loop.

737
00:32:52,050 --> 00:32:56,070
And do not hesitate to reach
out to us on email or Twitter.

738
00:32:56,070 --> 00:32:56,580
Thank you.

739
00:32:56,580 --> 00:32:59,630
[MUSIC PLAYING]

740
00:32:59,630 --> 00:33:07,093